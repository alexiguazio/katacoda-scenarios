Note: Wait until the (pip) install is complete and the MLRun DB/API service has started before moving forward!

> This setup includes a local MLRun and Nuclio service. To work with a remote k8s cluster or the [**Iguazio Platform**](https://www.iguazio.com/) you'll need to edit the 
> `remote.env`{{open}} file with the remote address and credentials and swap the environment using the call: 
> `mlrun.set_env_from_file("remote.env")` 
 
- Define a new project (for our functions):

`project = mlrun.new_project("coda-[[HOST_SUBDOMAIN]]")`{{execute}}

- Create a new serving function from source code (`serving.py`{{open}}):

`serving_fn = mlrun.code_to_function('serving', filename="serving.py", kind='serving',image='mlrun/mlrun')`{{execute}}

- Add a model to the serving function (using a sample iris model):

```python
model_file = mlrun.get_sample_path("models/iris/model.pkl")
serving_fn.add_model('my_model',model_path=model_file, class_name='ClassifierModel')
```{{execute}}

> MLRun accepts simple model files (.pkl, ..) or MLRun Model Artifact objects (generated by the training), 
> **Model Artifacts** include the model parameters, schema, statistics, and metadata used to properly initialize and monitor the models.
